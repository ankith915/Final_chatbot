{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "from fastapi import HTTPException, status\n",
    "from pyTigerGraph import TigerGraphConnection\n",
    "# from app.common.config import db_config\n",
    "\n",
    "# Define roles with full access\n",
    "ALLOWED_ROLES = {'superuser', 'globaldesigner', 'admin'}\n",
    "\n",
    "user_role_pattern = r'- Name:\\s+(.+?)\\s+- Global Roles:\\s+(.+?)\\s+-'\n",
    "\n",
    "def get_user_role(username: str, password: str, conn=None) -> tuple[list[str], TigerGraphConnection]:\n",
    "    if conn is None:\n",
    "        conn = TigerGraphConnection(\n",
    "            # host=db_config[\"hostname\"], graphname=\"\", username=username, password=password\n",
    "            host=\"tigergraph_host\",\n",
    "            graphname=\"\", username=username, password=password\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # parse user info\n",
    "        info = conn.gsql(\"SHOW USER\")\n",
    "        user_roles = {}\n",
    "        # print (info)\n",
    "        for match in re.finditer(user_role_pattern, info):\n",
    "            name = match.group(1).strip()\n",
    "            global_roles = match.group(2).strip()\n",
    "            user_roles[name] = global_roles\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Incorrect username or password\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    return user_roles, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_roles, conn = get_user_role(username=\"USERNAME\", password=\"PASSWORD\")\n",
    "\n",
    "for user, roles in user_roles.items():\n",
    "    print (f\"{user}: {roles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(username, password):\n",
    "    \"\"\"Create headers with Base64 encoded credentials.\"\"\"\n",
    "    credentials = f\"{username}:{password}\"\n",
    "    encoded_credentials = base64.b64encode(credentials.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    return {\n",
    "        'accept': 'application/json',\n",
    "        'Authorization': f'Basic {encoded_credentials}'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_conversation_ids(username, password):\n",
    "    \"\"\"Fetch conversation IDs for a given user.\"\"\"\n",
    "    headers = create_headers(username, password)\n",
    "    user_url = f'http://0.0.0.0:8000/ui/user/{username}'\n",
    "    \n",
    "    response = requests.get(user_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [item['conversation_id'] for item in data]\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_conversation_ids(\"supportai\", \"supportai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_data(username, password, conversation_id):\n",
    "    \"\"\"Fetch conversation data for a given conversation ID.\"\"\"\n",
    "    headers = create_headers(username, password)\n",
    "    conversation_url = f'http://0.0.0.0:8000/ui/conversation/{conversation_id}'\n",
    "    \n",
    "    response = requests.get(conversation_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # return [\n",
    "        #     {\n",
    "        #         \"id\": message[\"id\"],\n",
    "        #         \"message_id\": message[\"message_id\"],\n",
    "        #         \"parent_id\": message[\"parent_id\"],\n",
    "        #         \"role\": message[\"role\"],\n",
    "        #         \"content\": message.get(\"content\", \"\"),\n",
    "        #         \"feedback\": message.get(\"feedback\", \"\")\n",
    "        #     }\n",
    "        #     for message in data\n",
    "        # ]\n",
    "        \n",
    "        # Create dictionaries to hold user questions and system answers\n",
    "        questions = {message[\"message_id\"]: message for message in data if message[\"role\"] == \"user\"}\n",
    "        answers = {message[\"parent_id\"]: message for message in data if message[\"role\"] == \"system\"}\n",
    "        \n",
    "        # return questions, answers\n",
    "        # Organize into Q&A pairs\n",
    "        qa_pairs = []\n",
    "        for q_id, question in questions.items():\n",
    "            if q_id in answers:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": question[\"content\"],\n",
    "                    \"answer\": answers[q_id][\"content\"],\n",
    "                    \"feedback\": answers[q_id][\"feedback\"]\n",
    "                })\n",
    "        \n",
    "        return qa_pairs\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_user_role(username, password, user_roles, conversation_id=None):\n",
    "    \"\"\"Check user role and permissions for accessing conversations.\"\"\"\n",
    "    if username not in user_roles:\n",
    "        return \"User does not exist in the database.\"\n",
    "    \n",
    "    if user_roles.get(username) in {'superuser', 'globaldesigner', 'admin'}:\n",
    "        return True  # Allow access for superusers, global designers, and admins\n",
    "    elif conversation_id:\n",
    "        # For non-superuser roles, check if the conversation belongs to the user\n",
    "        conversation_ids = get_user_conversation_ids(username, password)\n",
    "        if conversation_id in conversation_ids:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_conversations(username, password, user_roles, conversation_id=None):\n",
    "    \"\"\"Fetch conversations based on user roles and permissions.\"\"\"\n",
    "    permission_check = check_user_role(username, password, user_roles)\n",
    "    if permission_check is not True:\n",
    "        return permission_check  # Return the error message if permissions are insufficient or user does not exist\n",
    "\n",
    "    if conversation_id:\n",
    "            # Fetch a specific conversation\n",
    "            data = get_conversation_data(username, password, conversation_id)\n",
    "            if data:\n",
    "                return data\n",
    "            else:\n",
    "                return \"Conversation not found or could not be retrieved.\"\n",
    "            \n",
    "    else:\n",
    "        # Fetch all conversations\n",
    "        conversation_ids = get_user_conversation_ids(username, password)\n",
    "        conversations = {}\n",
    "        \n",
    "        for conv_id in conversation_ids:\n",
    "            data = get_conversation_data(username, password, conv_id)\n",
    "            if data:\n",
    "                conversations[conv_id] = data\n",
    "    \n",
    "        return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_data = fetch_user_conversations(\"USERNAME\", \"PASSWORD\", user_roles, \"CONVO_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback_stats(conversation_data):\n",
    "    # Initialize counters\n",
    "    feedback_counts = {'No Feedback': 0, 'Thumbs Up': 0, 'Thumbs Down': 0}\n",
    "    total_entries = len(conversation_data)\n",
    "    \n",
    "    # Count feedback occurrences\n",
    "    for entry in conversation_data:\n",
    "        feedback = entry['feedback']\n",
    "        if feedback == 0:\n",
    "            feedback_counts['No Feedback'] += 1\n",
    "        elif feedback == 1:\n",
    "            feedback_counts['Thumbs Up'] += 1\n",
    "        elif feedback == 2:\n",
    "            feedback_counts['Thumbs Down'] += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    feedback_percentages = {k: (v / total_entries) * 100 for k, v in feedback_counts.items()}\n",
    "    \n",
    "    return feedback_counts, feedback_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feedback stats\n",
    "feedback_counts, feedback_percentages = get_feedback_stats(conversation_data)\n",
    "\n",
    "print(\"Feedback Counts:\", feedback_counts)\n",
    "print(\"Feedback Percentages:\", feedback_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'APIKEY'\n",
    "\n",
    "# def summarize_text(text):\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4\", \n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{text}\"}\n",
    "#         ],\n",
    "#         max_tokens=100,  # Adjust the number of tokens as needed\n",
    "#         temperature=0.1\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "def classify_text(text, labels):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that classifies text into categories.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following text into one of these categories: {', '.join(labels)}\\n\\nText:\\n{text} and just return the labels only\"}\n",
    "        ],\n",
    "        max_tokens=50,  # Adjust the number of tokens as needed\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_messages = [item['question'] for item in conversation_data]\n",
    "\n",
    "# Define labels for classification\n",
    "labels = [\"Missing Information\", \"Incorrect Information\", \"Irrelevant Information\"]\n",
    "\n",
    "# Process each piece of conversation data\n",
    "# for i, text in enumerate(feedback_messages):\n",
    "#     # summary = summarize_text(text)\n",
    "#     classification = classify_text(text, labels)\n",
    "#     # print(f\"Conversation {i+1} Summary: {summary}\")\n",
    "#     print(f\"Message: {text}\")\n",
    "#     print(f\"Classification: {classification}\")\n",
    "#     print(\"---\")\n",
    "\n",
    "# Print the results in the desired format\n",
    "# Initialize a dictionary to store classified issues\n",
    "classified_issues = {category: [] for category in labels}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for message in feedback_messages:\n",
    "    result = classify_text(message, labels)\n",
    "    results[message] = result\n",
    "# Categorize each piece of text\n",
    "for text, category in results.items()   :\n",
    "    if category in classified_issues:\n",
    "        classified_issues[category].append(text)\n",
    "    else:\n",
    "        classified_issues['Other'].append(text)\n",
    "\n",
    "# Print the results in the desired format\n",
    "print(\"\\nCategorized Issues:\\n\")\n",
    "for category in labels:\n",
    "    if classified_issues[category]:\n",
    "        print(f\"{category}:\")\n",
    "        for issue in classified_issues[category]:\n",
    "            print(f\" - {issue}\")\n",
    "        print()  # Add a newline for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load summarization and zero-shot classification pipelines\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def analyze_negative_feedback_with_llm(conversation_data):\n",
    "    negative_feedback_entries = [entry for entry in conversation_data if entry['feedback'] == 2]\n",
    "    \n",
    "    # Define categories for classification\n",
    "    categories = [\"Missing Information\", \"Incorrect Information\", \"Irrelevant Information\", \"Other\"]\n",
    "    \n",
    "    summarized_issues = []\n",
    "    categorized_issues = {category: [] for category in categories}\n",
    "    \n",
    "    for entry in negative_feedback_entries:\n",
    "        content = entry['content']\n",
    "\n",
    "        # Dynamically set max_length based on input length\n",
    "        input_length = len(content.split())\n",
    "        max_length = max(10, int(input_length * 0.8))\n",
    "        \n",
    "        # Summarize the content\n",
    "        summary = summarizer(content, max_length=max_length, min_length=5, do_sample=False)[0]['summary_text']\n",
    "        summarized_issues.append(summary)\n",
    "        \n",
    "        # Classify the content into categories\n",
    "        classification = classifier(content, candidate_labels=categories)\n",
    "        top_category = classification['labels'][0]\n",
    "        categorized_issues[top_category].append(content)\n",
    "    \n",
    "    return summarized_issues, categorized_issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the analysis\n",
    "summarized_issues, categorized_issues = analyze_negative_feedback_with_llm(conversation_data)\n",
    "\n",
    "# Print the summarized issues\n",
    "# print(\"Summarized Issues:\")\n",
    "# for summary in summarized_issues:\n",
    "#     print(summary)\n",
    "\n",
    "# Print categorized issues and identify incorrect information\n",
    "print(\"\\nCategorized Issues:\")\n",
    "for category, contents in categorized_issues.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for content in contents:\n",
    "        print(f\" - {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Extract negative feedback messages\n",
    "feedback_messages = [item['question'] for item in conversation_data]\n",
    "\n",
    "# Custom tokenizer to handle specific patterns\n",
    "def custom_tokenizer(text):\n",
    "    # Split by non-alphanumeric characters\n",
    "    tokens = text.split()\n",
    "    # Remove numbers and tokens less than 3 characters long\n",
    "    tokens = [token for token in tokens if not token.isdigit() and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Vectorize the feedback messages with custom tokenizer\n",
    "vectorizer = CountVectorizer(stop_words='english', tokenizer=custom_tokenizer)\n",
    "X = vectorizer.fit_transform(feedback_messages)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display topics\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "num_top_words = 5\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract feedback messages\n",
    "feedback_messages = [item['question'] for item in conversation_data]\n",
    "\n",
    "# Vectorize the feedback messages\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(feedback_messages)\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 4  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print clustering results\n",
    "print(\"Clustering results:\")\n",
    "for i in range(num_clusters):\n",
    "    cluster_label = f\"Cluster {i}:\"\n",
    "    cluster_messages = [feedback_messages[j] for j in range(len(kmeans.labels_)) if kmeans.labels_[j] == i]\n",
    "    print(cluster_label)\n",
    "    for message in cluster_messages:\n",
    "        print(f\"- {message}\")\n",
    "\n",
    "# Print cluster centroids (important words for each cluster)\n",
    "print(\"\\nCluster centroids (top words per cluster):\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i} words:\", end='')\n",
    "    for ind in order_centroids[i, :10]:  # Print top 10 words per cluster\n",
    "        print(f' {terms[ind]}', end='')\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
